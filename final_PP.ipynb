{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJSvJwS24wn0"
   },
   "source": [
    "# **Final Project**\n",
    "\n",
    "## **Problem stament :**     \n",
    "\n",
    "The widespread dissemination of fake news and propaganda presents serious societal risks, including the erosion of public trust, political polarization, manipulation of elections, and the spread of harmful misinformation during crises such as pandemics or conflicts. From an NLP perspective, detecting fake news is fraught with challenges. Linguistically, fake news often mimics the tone and structure of legitimate journalism, making it difficult to distinguish using surface-level features. The absence of reliable and up-to-date labeled datasets, especially across multiple languages and regions, hampers the effectiveness of supervised learning models. Additionally, the dynamic and adversarial nature of misinformation means that malicious actors constantly evolve their language and strategies to bypass detection systems. Cultural context, sarcasm, satire, and implicit bias further complicate automated analysis. Moreover, NLP models risk amplifying biases present in training data, leading to unfair classifications and potential censorship of legitimate content. These challenges underscore the need for cautious, context-aware approaches, as the failure to address them can inadvertently contribute to misinformation, rather than mitigate it.\n",
    "\n",
    "\n",
    "\n",
    "Use datasets in link : https://drive.google.com/drive/folders/1mrX3vPKhEzxG96OCPpCeh9F8m_QKCM4z?usp=sharing\n",
    "to complete requirement.\n",
    "\n",
    "## **About dataset:**\n",
    "\n",
    "* **True Articles**:\n",
    "\n",
    "  * **File**: `MisinfoSuperset_TRUE.csv`\n",
    "  * **Sources**:\n",
    "\n",
    "    * Reputable media outlets like **Reuters**, **The New York Times**, **The Washington Post**, etc.\n",
    "\n",
    "* **Fake/Misinformation/Propaganda Articles**:\n",
    "\n",
    "  * **File**: `MisinfoSuperset_FAKE.csv`\n",
    "  * **Sources**:\n",
    "\n",
    "    * **American right-wing extremist websites** (e.g., Redflag Newsdesk, Breitbart, Truth Broadcast Network)\n",
    "    * **Public dataset** from:\n",
    "\n",
    "      * Ahmed, H., Traore, I., & Saad, S. (2017): \"Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques\" *(Springer LNCS 10618)*\n",
    "\n",
    "\n",
    "\n",
    "## **Requirement**\n",
    "\n",
    "A team consisting of three members must complete a project that involves applying the methods learned from the beginning of the course up to the present. The team is expected to follow and document the entire machine learning workflow, which includes the following steps:\n",
    "\n",
    "1. **Data Preprocessing**: Clean and prepare the dataset,etc.\n",
    "\n",
    "2. **Exploratory Data Analysis (EDA)**: Explore and visualize the data.\n",
    "\n",
    "3. **Model Building**: Select and build one or more machine learning models suitable for the problem at hand.\n",
    "\n",
    "4. **Hyperparameter set up**: Set and adjust the model's hyperparameters using appropriate methods to improve performance.\n",
    "\n",
    "5. **Model Training**: Train the model(s) on the training dataset.\n",
    "\n",
    "6. **Performance Evaluation**: Evaluate the trained model(s) using appropriate metrics (e.g., accuracy, precision, recall, F1-score, confusion matrix, etc.) and validate their performance on unseen data.\n",
    "\n",
    "7. **Conclusion**: Summarize the results, discuss the model's strengths and weaknesses, and suggest possible improvements or future work.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZlAJs4n4yfT"
   },
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-17T12:48:15.361536Z",
     "iopub.status.busy": "2025-06-17T12:48:15.361048Z",
     "iopub.status.idle": "2025-06-17T12:48:19.666832Z",
     "shell.execute_reply": "2025-06-17T12:48:19.665950Z",
     "shell.execute_reply.started": "2025-06-17T12:48:15.361515Z"
    },
    "id": "Sag__UElw91_",
    "outputId": "d0889c16-7acc-42c1-9324-bc1b945c4284",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-17T12:48:19.668674Z",
     "iopub.status.busy": "2025-06-17T12:48:19.668335Z",
     "iopub.status.idle": "2025-06-17T12:48:24.911820Z",
     "shell.execute_reply": "2025-06-17T12:48:24.911175Z",
     "shell.execute_reply.started": "2025-06-17T12:48:19.668651Z"
    },
    "id": "KA7vmonq423g",
    "outputId": "883fb31b-4191-4173-e30e-21853d72c5cc",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=a567161af394d34dbce76589b0a745cf27884ae51324fdda3d26a477f44a6ae9\n",
      "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-06-17T12:48:24.913629Z",
     "iopub.status.busy": "2025-06-17T12:48:24.913421Z",
     "iopub.status.idle": "2025-06-17T12:48:27.856522Z",
     "shell.execute_reply": "2025-06-17T12:48:27.855825Z",
     "shell.execute_reply.started": "2025-06-17T12:48:24.913610Z"
    },
    "id": "A_8TpEQRINOi",
    "outputId": "1979769d-be9b-4817-ab95-34e2fc1e1b52",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (2.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILypBVMaQLUq"
   },
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:42.502404Z",
     "iopub.status.busy": "2025-06-17T12:49:42.501671Z",
     "iopub.status.idle": "2025-06-17T12:49:42.508627Z",
     "shell.execute_reply": "2025-06-17T12:49:42.508022Z",
     "shell.execute_reply.started": "2025-06-17T12:49:42.502373Z"
    },
    "id": "qmsu51HB45Iz",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import html\n",
    "import os\n",
    "import quopri\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect, LangDetectException\n",
    "from tqdm import tqdm\n",
    "\n",
    "import contractions\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    XLNetForSequenceClassification,\n",
    "    get_scheduler,\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import get_scheduler\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:46.203454Z",
     "iopub.status.busy": "2025-06-17T12:49:46.203191Z",
     "iopub.status.idle": "2025-06-17T12:49:48.744080Z",
     "shell.execute_reply": "2025-06-17T12:49:48.743472Z",
     "shell.execute_reply.started": "2025-06-17T12:49:46.203435Z"
    },
    "id": "7PyECqZ-46fx",
    "outputId": "1247cdfe-f834-4738-eaa4-f52c5f001478",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34970</th>\n",
       "      <td>34970</td>\n",
       "      <td>Most conservatives who oppose marriage equalit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34971</th>\n",
       "      <td>34971</td>\n",
       "      <td>The freshman senator from Georgia quoted scrip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34972</th>\n",
       "      <td>34972</td>\n",
       "      <td>The State Department told the Republican Natio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34973</th>\n",
       "      <td>34973</td>\n",
       "      <td>ADDIS ABABA, Ethiopia ‚ÄîPresident Obama convene...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34974</th>\n",
       "      <td>34974</td>\n",
       "      <td>Jeb Bush Is Suddenly Attacking Trump. Here's W...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34975 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text\n",
       "0               0  The head of a conservative Republican faction ...\n",
       "1               1  Transgender people will be allowed for the fir...\n",
       "2               2  The special counsel investigation of links bet...\n",
       "3               3  Trump campaign adviser George Papadopoulos tol...\n",
       "4               4  President Donald Trump called on the U.S. Post...\n",
       "...           ...                                                ...\n",
       "34970       34970  Most conservatives who oppose marriage equalit...\n",
       "34971       34971  The freshman senator from Georgia quoted scrip...\n",
       "34972       34972  The State Department told the Republican Natio...\n",
       "34973       34973  ADDIS ABABA, Ethiopia ‚ÄîPresident Obama convene...\n",
       "34974       34974  Jeb Bush Is Suddenly Attacking Trump. Here's W...\n",
       "\n",
       "[34975 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df = pd.read_csv(\"/kaggle/input/misinfo/DataSet_Misinfo_TRUE.csv\")\n",
    "true_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:48.745344Z",
     "iopub.status.busy": "2025-06-17T12:49:48.745119Z",
     "iopub.status.idle": "2025-06-17T12:49:50.939363Z",
     "shell.execute_reply": "2025-06-17T12:49:50.938792Z",
     "shell.execute_reply.started": "2025-06-17T12:49:48.745329Z"
    },
    "id": "FdFf6oSg475X",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43637</th>\n",
       "      <td>44422</td>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43638</th>\n",
       "      <td>44423</td>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43639</th>\n",
       "      <td>44424</td>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43640</th>\n",
       "      <td>44425</td>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43641</th>\n",
       "      <td>44426</td>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43642 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text\n",
       "0               0  Donald Trump just couldn t wish all Americans ...\n",
       "1               1  House Intelligence Committee Chairman Devin Nu...\n",
       "2               2  On Friday, it was revealed that former Milwauk...\n",
       "3               3  On Christmas day, Donald Trump announced that ...\n",
       "4               4  Pope Francis used his annual Christmas Day mes...\n",
       "...           ...                                                ...\n",
       "43637       44422  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...\n",
       "43638       44423  The Ukrainian coup d'etat cost the US nothing ...\n",
       "43639       44424  The European Parliament falsifies history by d...\n",
       "43640       44425  The European Parliament falsifies history by d...\n",
       "43641       44426  A leading FSB officer, Segey Beseda, said duri...\n",
       "\n",
       "[43642 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_df = pd.read_csv(\"/kaggle/input/misinfo/DataSet_Misinfo_FAKE.csv\")\n",
    "fake_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:50.940919Z",
     "iopub.status.busy": "2025-06-17T12:49:50.940338Z",
     "iopub.status.idle": "2025-06-17T12:49:50.950883Z",
     "shell.execute_reply": "2025-06-17T12:49:50.950203Z",
     "shell.execute_reply.started": "2025-06-17T12:49:50.940895Z"
    },
    "id": "XqMBo7ly4-bE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Delete order column\n",
    "true_df = true_df.drop('Unnamed: 0', axis=1)\n",
    "fake_df = fake_df.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-17T12:48:54.099022Z",
     "iopub.status.idle": "2025-06-17T12:48:54.099280Z",
     "shell.execute_reply": "2025-06-17T12:48:54.099166Z",
     "shell.execute_reply.started": "2025-06-17T12:48:54.099154Z"
    },
    "id": "orKgADru4_8N",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "fake_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9B71gKh45BQR"
   },
   "outputs": [],
   "source": [
    "true_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oyr1ehMs5Cj6"
   },
   "outputs": [],
   "source": [
    "fake_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UedEPJBX5FEw"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ke2XC07Z5GkZ"
   },
   "source": [
    "- X·ª≠ l√Ω gi√° tr·ªã null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TzZQEtPk5Lku"
   },
   "outputs": [],
   "source": [
    "true_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q8BC8NCR5M3x"
   },
   "outputs": [],
   "source": [
    "fake_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:54.745729Z",
     "iopub.status.busy": "2025-06-17T12:49:54.745466Z",
     "iopub.status.idle": "2025-06-17T12:49:54.759000Z",
     "shell.execute_reply": "2025-06-17T12:49:54.758190Z",
     "shell.execute_reply.started": "2025-06-17T12:49:54.745709Z"
    },
    "id": "njIAkAPN5NTY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "true_df = true_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2cz30lw5K4x"
   },
   "source": [
    "- X·ª≠ l√Ω gi√° tr·ªã duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_8t6Blo5QBD"
   },
   "outputs": [],
   "source": [
    "true_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSaZ_iud5R7u"
   },
   "outputs": [],
   "source": [
    "fake_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:57.264925Z",
     "iopub.status.busy": "2025-06-17T12:49:57.264300Z",
     "iopub.status.idle": "2025-06-17T12:49:57.394492Z",
     "shell.execute_reply": "2025-06-17T12:49:57.393569Z",
     "shell.execute_reply.started": "2025-06-17T12:49:57.264901Z"
    },
    "id": "JNqDdvw55TSG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "true_df = true_df.drop_duplicates()\n",
    "fake_df = fake_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpvYPH0eJm89"
   },
   "source": [
    "- Th√™m label v√† g·ªôp 2 t·∫≠p d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:49:59.295954Z",
     "iopub.status.busy": "2025-06-17T12:49:59.295283Z",
     "iopub.status.idle": "2025-06-17T12:49:59.307924Z",
     "shell.execute_reply": "2025-06-17T12:49:59.307355Z",
     "shell.execute_reply.started": "2025-06-17T12:49:59.295928Z"
    },
    "id": "C6tDHB6eJnYB",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The head of a conservative Republican faction ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transgender people will be allowed for the fir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The special counsel investigation of links bet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump campaign adviser George Papadopoulos tol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>President Donald Trump called on the U.S. Post...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68599</th>\n",
       "      <td>Apparently, the new Kyiv government is in a hu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68600</th>\n",
       "      <td>The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68601</th>\n",
       "      <td>The Ukrainian coup d'etat cost the US nothing ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68602</th>\n",
       "      <td>The European Parliament falsifies history by d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68603</th>\n",
       "      <td>A leading FSB officer, Segey Beseda, said duri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68604 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      The head of a conservative Republican faction ...      1\n",
       "1      Transgender people will be allowed for the fir...      1\n",
       "2      The special counsel investigation of links bet...      1\n",
       "3      Trump campaign adviser George Papadopoulos tol...      1\n",
       "4      President Donald Trump called on the U.S. Post...      1\n",
       "...                                                  ...    ...\n",
       "68599  Apparently, the new Kyiv government is in a hu...      0\n",
       "68600  The USA wants to divide Syria.\\r\\n\\r\\nGreat Br...      0\n",
       "68601  The Ukrainian coup d'etat cost the US nothing ...      0\n",
       "68602  The European Parliament falsifies history by d...      0\n",
       "68603  A leading FSB officer, Segey Beseda, said duri...      0\n",
       "\n",
       "[68604 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_df['label'] = 1\n",
    "fake_df['label'] = 0\n",
    "\n",
    "df = pd.concat([true_df, fake_df], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:50:01.892410Z",
     "iopub.status.busy": "2025-06-17T12:50:01.892148Z",
     "iopub.status.idle": "2025-06-17T12:50:01.912075Z",
     "shell.execute_reply": "2025-06-17T12:50:01.911516Z",
     "shell.execute_reply.started": "2025-06-17T12:50:01.892392Z"
    },
    "id": "928LxVh0Jo4h",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Former Russian economy minister Alexei Ulyukay...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Republicans were just given a leg up over Demo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This has to be one of the best remix videos ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In line with the new Language Law, Russian is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>JERUSALEM  ‚Äî   A day after approving the const...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68599</th>\n",
       "      <td>The Super Bowl had not yet begun and Trump fan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68600</th>\n",
       "      <td>U.S. House Republicans on Friday won passage o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68601</th>\n",
       "      <td>Share on Facebook Share on Twitter Known to th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68602</th>\n",
       "      <td>A New Jersey man who worked at the World Trade...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68603</th>\n",
       "      <td>Turkey and Iran have agreed to discuss within ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68604 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "0      Former Russian economy minister Alexei Ulyukay...      1\n",
       "1      Republicans were just given a leg up over Demo...      0\n",
       "2      This has to be one of the best remix videos ev...      0\n",
       "3      In line with the new Language Law, Russian is ...      0\n",
       "4      JERUSALEM  ‚Äî   A day after approving the const...      1\n",
       "...                                                  ...    ...\n",
       "68599  The Super Bowl had not yet begun and Trump fan...      0\n",
       "68600  U.S. House Republicans on Friday won passage o...      1\n",
       "68601  Share on Facebook Share on Twitter Known to th...      0\n",
       "68602  A New Jersey man who worked at the World Trade...      1\n",
       "68603  Turkey and Iran have agreed to discuss within ...      1\n",
       "\n",
       "[68604 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True) # Shuffle dataset\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb4Hormg5aWf"
   },
   "source": [
    "* Ki·ªÉm tra imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-17T12:48:54.107613Z",
     "iopub.status.idle": "2025-06-17T12:48:54.107871Z",
     "shell.execute_reply": "2025-06-17T12:48:54.107772Z",
     "shell.execute_reply.started": "2025-06-17T12:48:54.107760Z"
    },
    "id": "tvMmWtg25b-P",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "403QUP4q5e6s"
   },
   "source": [
    "=> D·ªØ li·ªáu kh√¥ng b·ªã imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vYaFeDU5kO0"
   },
   "source": [
    "- X·ª≠ l√Ω c√°c vƒÉn b·∫£n kh√¥ng ph·∫£i l√† ti·∫øng Anh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:50:04.616533Z",
     "iopub.status.busy": "2025-06-17T12:50:04.615861Z",
     "iopub.status.idle": "2025-06-17T12:56:39.844239Z",
     "shell.execute_reply": "2025-06-17T12:56:39.843425Z",
     "shell.execute_reply.started": "2025-06-17T12:50:04.616509Z"
    },
    "id": "v7gyy9NYOr_d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  label     lang\n",
      "127                                   Florida for Trump!      0  unknown\n",
      "253    0 –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ 0 –ø–æ–¥–µ–ª–∏–ª–∏—Å—å –§–æ—Ç–æ: AP \\n–ö–æ–º–º–µ–Ω—Ç...      0       ru\n",
      "294    0 –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ 7 –ø–æ–¥–µ–ª–∏–ª–∏—Å—å \\n\"–≠—Ç–æ –ø–æ–ª–Ω—ã–π –±—Ä–µ–¥...      0       ru\n",
      "297    +++ Beim Jupiter! Spuren r√∂mischer Zivilisatio...      0       de\n",
      "433    0 –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ 0 –ø–æ–¥–µ–ª–∏–ª–∏—Å—å \\n23 –æ–∫—Ç—è–±—Ä—è –≤ –ù–∏–∂...      0       ru\n",
      "...                                                  ...    ...      ...\n",
      "68104  Mittwoch, 16. November 2016 Neue App ruft auto...      0       de\n",
      "68347  +++ Muhten ihm einiges zu: Bauer soll Streit u...      0       de\n",
      "68388  ‚Äî The Sun (@TheSun) 23. November 2016 Laut Fer...      0       de\n",
      "68406                       President Obama is a Muslim.      0       ca\n",
      "68444  –°—Ç—Ä–∞–Ω–∞: –ö–∏—Ç–∞–π –ó–∞—è–≤–ª–µ–Ω–∏—è –ö–ù–î–† –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ —Å–≤–æ–µ...      0       ru\n",
      "\n",
      "[646 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "def safe_detect(x):\n",
    "    if isinstance(x, str) and x.strip() and len(x.strip()) > 20:\n",
    "        try:\n",
    "            return detect(x)\n",
    "        except LangDetectException:\n",
    "            return 'unknown'\n",
    "    return 'unknown'\n",
    "\n",
    "df['lang'] = df['text'].apply(safe_detect)\n",
    "non_english = df[df['lang'] != 'en']\n",
    "print(non_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:39.845920Z",
     "iopub.status.busy": "2025-06-17T12:56:39.845450Z",
     "iopub.status.idle": "2025-06-17T12:56:39.853521Z",
     "shell.execute_reply": "2025-06-17T12:56:39.852792Z",
     "shell.execute_reply.started": "2025-06-17T12:56:39.845902Z"
    },
    "id": "BC_W0QPzOxVy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"lang\", axis=1) # X√≥a c·ªôt ph·ª• sau khi x·ª≠ l√Ω"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-m74wad8O_sX"
   },
   "source": [
    "=> Kh√¥ng x√≥a c√°c d√≤ng vƒÉn b·∫£n kh√¥ng ph·∫£i ti·∫øng Anh v√¨ label 0 - fake news chi·∫øm ƒëa s·ªë"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR7Z3W7_5srF"
   },
   "source": [
    "- X·ª≠ l√Ω c√°c vƒÉn b·∫£n v·ªõi s·ªë t·ª´ √≠t h∆°n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:39.854468Z",
     "iopub.status.busy": "2025-06-17T12:56:39.854237Z",
     "iopub.status.idle": "2025-06-17T12:56:41.606767Z",
     "shell.execute_reply": "2025-06-17T12:56:41.606062Z",
     "shell.execute_reply.started": "2025-06-17T12:56:39.854447Z"
    },
    "id": "qEqtwUXC5uJ8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text  label\n",
      "127                                 Florida for Trump!      0\n",
      "288    A MUST watch video!https://youtu.be/-5Z-jJ2Z4bU      0\n",
      "772                                               Cool      0\n",
      "965                    That would be unconstitutional.      0\n",
      "1115                   Around 120,000 displaced people      1\n",
      "...                                                ...    ...\n",
      "67547                           TRUMP VICTORY FOR SURE      0\n",
      "67689                                        Brilliant      0\n",
      "67766                                  Good guy.\\nüëçüëçüëçüëç      0\n",
      "67797      https://www.youtube.com/watch?v=gqxwF-TeYas      0\n",
      "67834                                        Horseshit      0\n",
      "\n",
      "[170 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# ƒê·∫øm s·ªë t·ª´ trong m·ªói d√≤ng\n",
    "short_texts = df[df['text'].apply(lambda x: len(str(x).split()) < 5)]\n",
    "\n",
    "# In ra c√°c d√≤ng n√†y\n",
    "print(short_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:41.608443Z",
     "iopub.status.busy": "2025-06-17T12:56:41.608247Z",
     "iopub.status.idle": "2025-06-17T12:56:41.615446Z",
     "shell.execute_reply": "2025-06-17T12:56:41.614931Z",
     "shell.execute_reply.started": "2025-06-17T12:56:41.608428Z"
    },
    "id": "bfxfwHQP5v6A",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1115</th>\n",
       "      <td>Around 120,000 displaced people</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20229</th>\n",
       "      <td>Republican Congressman Will Hurd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24713</th>\n",
       "      <td>Ted Cruz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26250</th>\n",
       "      <td>Four U.S. senators</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28034</th>\n",
       "      <td>‚ÄúOn 1/20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31892</th>\n",
       "      <td>No.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40323</th>\n",
       "      <td>(Reuters)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57596</th>\n",
       "      <td>Jan 29 (Reuters)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65117</th>\n",
       "      <td>advertisement</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text  label\n",
       "1115    Around 120,000 displaced people      1\n",
       "20229  Republican Congressman Will Hurd      1\n",
       "24713                          Ted Cruz      1\n",
       "26250                Four U.S. senators      1\n",
       "28034                          ‚ÄúOn 1/20      1\n",
       "31892                               No.      1\n",
       "40323                         (Reuters)      1\n",
       "57596                  Jan 29 (Reuters)      1\n",
       "65117                     advertisement      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_texts[short_texts['label']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnSviFeV5y5u"
   },
   "source": [
    "=> V√¨ c√°c d√≤ng c√≥ text d∆∞·ªõi 5 k√≠ t·ª± kh√¥ng mang nhi·ªÅu √Ω nghƒ©a n√™n lo·∫°i b·ªè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:41.616893Z",
     "iopub.status.busy": "2025-06-17T12:56:41.616207Z",
     "iopub.status.idle": "2025-06-17T12:56:43.386130Z",
     "shell.execute_reply": "2025-06-17T12:56:43.385368Z",
     "shell.execute_reply.started": "2025-06-17T12:56:41.616873Z"
    },
    "id": "A5p6Ku4H5xvq",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Lo·∫°i b·ªè c√°c d√≤ng c√≥ s·ªë t·ª´ < 5\n",
    "df = df[df['text'].apply(lambda x: len(str(x).split()) >= 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4yBEgaak51Rj"
   },
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd9m-Xae52wF"
   },
   "source": [
    "- L√†m s·∫°ch vƒÉn b·∫£n (lower, b·ªè d·∫•u c√¢u, stopwords, stemming...) + Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:43.387104Z",
     "iopub.status.busy": "2025-06-17T12:56:43.386881Z",
     "iopub.status.idle": "2025-06-17T12:56:43.392301Z",
     "shell.execute_reply": "2025-06-17T12:56:43.391742Z",
     "shell.execute_reply.started": "2025-06-17T12:56:43.387081Z"
    },
    "id": "BqU7Oo2I54mM",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first running\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:43.393159Z",
     "iopub.status.busy": "2025-06-17T12:56:43.392981Z",
     "iopub.status.idle": "2025-06-17T12:56:43.406584Z",
     "shell.execute_reply": "2025-06-17T12:56:43.405925Z",
     "shell.execute_reply.started": "2025-06-17T12:56:43.393145Z"
    },
    "id": "ykzDhSM057Jk",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:56:43.407691Z",
     "iopub.status.busy": "2025-06-17T12:56:43.407474Z",
     "iopub.status.idle": "2025-06-17T12:59:54.054781Z",
     "shell.execute_reply": "2025-06-17T12:59:54.054158Z",
     "shell.execute_reply.started": "2025-06-17T12:56:43.407673Z"
    },
    "id": "K0AWDT2g58rk",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        former russian economy minister alexei ulyukay...\n",
       "1        republicans were just given a leg up over demo...\n",
       "2        this has to be one of the best remix videos ev...\n",
       "3        in line with the new language law, russian is ...\n",
       "4        jerusalem ‚Äî a day after approving the construc...\n",
       "                               ...                        \n",
       "68599    the super bowl had not yet begun and trump fan...\n",
       "68600    u.s. house republicans on friday won passage o...\n",
       "68601    share on facebook share on twitter known to th...\n",
       "68602    a new jersey man who worked at the world trade...\n",
       "68603    turkey and iran have agreed to discuss within ...\n",
       "Name: clean_text, Length: 68434, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(row):\n",
    "    row = str(row).lower()\n",
    "\n",
    "    # Remove email headers\n",
    "    row = re.sub(r'(?i)\\b(from|to|cc|bcc|subject|date|return-path|message-id|thread-topic|thread-index|content-type|mime-version|boundary|received|x-[\\w-]+):.*', ' ', row)\n",
    "\n",
    "    # Remove mailto links\n",
    "    row = re.sub(r'mailto:[^\\s]+', ' ', row)\n",
    "\n",
    "    # Decode quoted-printable\n",
    "    row = quopri.decodestring(row.encode('utf-8')).decode('utf-8', errors='ignore')\n",
    "\n",
    "    # Unescape HTML entities\n",
    "    row = html.unescape(row)\n",
    "\n",
    "    # Strip HTML tags\n",
    "    if '<' in row and '>' in row:\n",
    "        row = BeautifulSoup(row, \"lxml\").get_text()\n",
    "\n",
    "    # Normalize\n",
    "    row = re.sub(r'[\\t\\r\\n]', ' ', row)\n",
    "    row = re.sub(r'[_~+\\-]{2,}', ' ', row)\n",
    "    row = re.sub(r\"[<>()|&¬©√∏%\\[\\]\\\\~*\\$‚Ç¨¬£¬•]\", ' ', row)\n",
    "    row = re.sub(r\"\\\\x[0-9a-fA-F]{2}\", ' ', row)\n",
    "    row = re.sub(r'(https?://)([^/\\s]+)([^\\s]*)', r'\\2', row)\n",
    "    row = re.sub(r'[a-f0-9]{16,}', ' ', row)\n",
    "    row = re.sub(r'([.?!])[\\s]*\\1+', r'\\1', row)\n",
    "    row = re.sub(r'\\s+', ' ', row)\n",
    "\n",
    "    # Remove code-like keywords\n",
    "    row = re.sub(r'\\b(function|var|return|typeof|window|document|eval|\\.split)\\b', ' ', row)\n",
    "\n",
    "    # Remove programming symbols\n",
    "    row = re.sub(r'[{}=<>\\[\\]^~|`#@*]', ' ', row)\n",
    "\n",
    "    # Remove all emoji\n",
    "    row = emoji.replace_emoji(row, replace='')\n",
    "\n",
    "    # Cut code JS minify or base36 encode\n",
    "    code_gibberish = re.search(r'[a-z0-9]{20,}', row)\n",
    "    if code_gibberish and len(row) - code_gibberish.start() > 50:\n",
    "        row = row[:code_gibberish.start()]\n",
    "\n",
    "    # Cut off JS/CDATA tail\n",
    "    cutoff = re.search(\n",
    "        r'(//\\s*!?\\s*cdata|function\\s*\\(|var\\s+[a-zA-Z]|window\\s*\\.\\s*|document\\s*\\.\\s*|this\\s*\\.)',\n",
    "        row\n",
    "    )\n",
    "    if cutoff and len(row) - cutoff.start() > 10:\n",
    "        row = row[:cutoff.start()]\n",
    "\n",
    "    row = re.sub(r'!+\\s*cdata\\s*!+', ' ', row, flags=re.IGNORECASE)\n",
    "\n",
    "    return row.strip()\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "df['clean_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nW7Xan1n6VtR"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xGnctfh_NtL"
   },
   "source": [
    "## Label Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fL2B5McW6Xyy"
   },
   "outputs": [],
   "source": [
    "def plot_label_distribution(df, label_col):\n",
    "    ax = sns.countplot(x=label_col, data=df, hue=label_col, palette='pastel', dodge=False)\n",
    "\n",
    "    counts = df[label_col].value_counts().sort_index()\n",
    "    for x, y in enumerate(counts.values):\n",
    "        ax.text(x, y, f'{y}', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "    plt.legend(title='Label', labels=df[label_col].unique(),)\n",
    "    plt.title('Label Distribution')\n",
    "    plt.xlabel('Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "plot_label_distribution(df, 'label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXhTsZeJ_R8d"
   },
   "source": [
    "S·ª± ch√™nh l·ªách gi·ªØa hai nh√£n l√† r·∫•t nh·ªè (ch·ªâ 448 m·∫´u), cho th·∫•y t·∫≠p d·ªØ li·ªáu kh√° c√¢n b·∫±ng gi·ªØa hai l·ªõp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLOPAw1V_Sqc"
   },
   "source": [
    "## Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vXjwkukP_T7B"
   },
   "outputs": [],
   "source": [
    "df['word_count'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='word_count', hue='label', multiple='dodge', bins=20)\n",
    "plt.title('Distribution of Word Count in True vs Fake News')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEyoJfqw_WSS"
   },
   "source": [
    "ƒêa s·ªë c√°c b√†i vi·∫øt (kho·∫£ng 30000 b√†i) c√≥ s·ªë t·ª´ t·ª´ 0 ƒë·∫øn 5000, cho c·∫£ True News v√† Fake News, v·ªõi nh√£n 0 (True News) c√≥ ph·∫ßn v∆∞·ª£t tr·ªôi h∆°n. R·∫•t √≠t b√†i vi·∫øt c√≥ s·ªë t·ª´ v∆∞·ª£t qu√° 10000, cho th·∫•y ph√¢n b·ªë t·∫≠p trung ch·ªß y·∫øu ·ªü c√°c b√†i vi·∫øt ng·∫Øn ƒë·∫øn trung b√¨nh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BwaYi_K_X-o"
   },
   "outputs": [],
   "source": [
    "df['char_count'] = df['clean_text'].apply(lambda x: len(x))\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='char_count', hue='label', multiple='dodge', bins=20)\n",
    "plt.title('Distribution of Character Count in True vs Fake News')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwFLXrMf_Zp6"
   },
   "source": [
    "Ph·∫ßn l·ªõn c√°c b√†i vi·∫øt (kho·∫£ng 30000 b√†i) c√≥ s·ªë k√Ω t·ª± t·ª´ 0 ƒë·∫øn 20000, v·ªõi nh√£n 0 (True News) chi·∫øm ∆∞u th·∫ø. S·ªë l∆∞·ª£ng gi·∫£m m·∫°nh sau 20000 k√Ω t·ª±, th·ªÉ hi·ªán s·ª± t·∫≠p trung ·ªü c√°c b√†i vi·∫øt c√≥ s·ªë k√Ω t·ª± th·∫•p ƒë·∫øn trung b√¨nh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fj5Df8jG_cT9"
   },
   "source": [
    "## Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0iBB2-T_bYU"
   },
   "outputs": [],
   "source": [
    "true_words = ' '.join(df[df['label'] == 1]['clean_text']).split()\n",
    "true_words = set(true_words)\n",
    "\n",
    "fake_words = ' '.join(df[df['label'] == 0]['clean_text']).split()\n",
    "fake_words = set(fake_words)\n",
    "\n",
    "common_words = true_words.intersection(fake_words)\n",
    "\n",
    "unique_true_words = true_words - common_words\n",
    "unique_fake_words = fake_words - common_words\n",
    "\n",
    "print(f\"Number of common words between true and fake news: {len(common_words)}\")\n",
    "print(f\"Number of unique words in true news: {len(unique_true_words)}\")\n",
    "print(f\"Number of unique words in fake news: {len(unique_fake_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCRIrsUa_fHC"
   },
   "outputs": [],
   "source": [
    "true_word_freq = Counter(true_words)\n",
    "most_common_true = true_word_freq.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in most_common_true], y=[word[0] for word in most_common_true])\n",
    "plt.title('Top 20 Most Common Words in True News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qF3RZjSp_gxg"
   },
   "outputs": [],
   "source": [
    "fake_word_freq = Counter(fake_words)\n",
    "most_common_fake = fake_word_freq.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in most_common_fake], y=[word[0] for word in most_common_fake])\n",
    "plt.title('Top 20 Most Common Words in Fake News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAkLl7OI_ibc"
   },
   "source": [
    "C·∫£ hai ƒë·ªì th·ªã \"Top 20 Most Common Words in True News\" v√† \"Top 20 Most Common Words in Fake News\" ƒë·ªÅu th·ªÉ hi·ªán t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa c√°c t·ª´ ph·ªï bi·∫øn nh·∫•t trong t·ª´ng lo·∫°i tin t·ª©c. T·ª´ `the` d·∫´n ƒë·∫ßu v·ªõi t·∫ßn su·∫•t cao nh·∫•t trong c·∫£ hai tr∆∞·ªùng h·ª£p, ti·∫øp theo l√† `to`, `of`, v√† `and`, cho th·∫•y ƒë√¢y l√† c√°c t·ª´ ch·ª©c nƒÉng ph·ªï bi·∫øn. True News c√≥ t·∫ßn su·∫•t t·ªëi ƒëa kho·∫£ng 1 tri·ªáu, trong khi Fake News c√≥ t·∫ßn su·∫•t cao h∆°n ƒë√°ng k·ªÉ, l√™n ƒë·∫øn g·∫ßn 8 tri·ªáu, ph·∫£n √°nh m·∫≠t ƒë·ªô t·ª´ cao h∆°n trong Fake News."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXBuVyLV_kKP"
   },
   "outputs": [],
   "source": [
    "common_word_freq = Counter(common_words)\n",
    "most_common_shared = common_word_freq.most_common(20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in most_common_shared], y=[word[0] for word in most_common_shared])\n",
    "plt.title('Top 20 Most Common Words Shared Between True and Fake News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqVGKXVf_mUJ"
   },
   "outputs": [],
   "source": [
    "true_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(true_words))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(true_wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for True News')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HDWpfCgv_n9f"
   },
   "outputs": [],
   "source": [
    "fake_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(fake_words))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(fake_wordcloud, interpolation='bilinear')\n",
    "plt.title('Word Cloud for Fake News')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H15ZgvzC_qNI"
   },
   "source": [
    "C·∫£ hai ƒë·ªÅu c√≥ s·ª± xu·∫•t hi·ªán c·ªßa `trump` v√† `clinton` v·ªõi k√≠ch th∆∞·ªõc l·ªõn, nh∆∞ng Fake News c√≥ th√™m c√°c t·ª´ li√™n quan ƒë·∫øn ph∆∞∆°ng ti·ªán truy·ªÅn th√¥ng (nh∆∞ `twitter`, `youtube`, `video`) v√† t·ª´ c·∫£m x√∫c (nh∆∞ `good`, `attack`), g·ª£i √Ω s·ª± kh√°c bi·ªát v·ªÅ phong c√°ch v√† n·ªôi dung so v·ªõi True News ch·ªâ t·∫≠p trung v√†o c√°c thu·∫≠t ng·ªØ ch√≠nh tr·ªã v√† h√†nh ch√≠nh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G57yoDC_s1V"
   },
   "source": [
    "## n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w0VeIAS_uSC"
   },
   "outputs": [],
   "source": [
    "def get_top_n_grams(corpus, ngram_range=(2, 2), n=None):\n",
    "    vec = CountVectorizer(ngram_range=ngram_range, stop_words='english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcrPAUB0_vVU"
   },
   "outputs": [],
   "source": [
    "top_positive_unigrams = get_top_n_grams(df[df['label'] == 1]['clean_text'], ngram_range=(1, 1), n=20)\n",
    "top_negative_unigrams = get_top_n_grams(df[df['label'] == 0]['clean_text'], ngram_range=(1, 1), n=20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in top_positive_unigrams], y=[word[0] for word in top_positive_unigrams])\n",
    "plt.title('Top 20 Unigrams in True News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Unigrams')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in top_negative_unigrams], y=[word[0] for word in top_negative_unigrams])\n",
    "plt.title('Top 20 Unigrams in Fake News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Unigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzrBzmNh_yRN"
   },
   "source": [
    "Trong True News, `said` d·∫´n ƒë·∫ßu v·ªõi t·∫ßn su·∫•t cao nh·∫•t (g·∫ßn 160,000), theo sau l√† `trump`, `mr`, `president`, v√† `new`, cho th·∫•y s·ª± t·∫≠p trung v√†o ph√°t ng√¥n v√† c√°c nh√¢n v·∫≠t ch√≠nh tr·ªã. Trong Fake News, `trump` ƒë·ª©ng ƒë·∫ßu v·ªõi t·∫ßn su·∫•t v∆∞·ª£t tr·ªôi (g·∫ßn 80,000), ti·∫øp theo l√† `people`, `said`, `clinton` v√† `president`, ph·∫£n √°nh s·ª± ch√∫ tr·ªçng v√†o c√°c nh√¢n v·∫≠t ch√≠nh tr·ªã v√† c√¥ng ch√∫ng.\n",
    "\n",
    "True News c√≥ t·∫ßn su·∫•t t·ªïng th·ªÉ cao h∆°n (l√™n ƒë·∫øn 160,000), trong khi Fake News c√≥ ph·∫°m vi t·∫ßn su·∫•t th·∫•p h∆°n (t·ªëi ƒëa 80,000), nh∆∞ng danh s√°ch t·ª´ ƒëa d·∫°ng h∆°n v·ªõi c√°c thu·∫≠t ng·ªØ nh∆∞ `election` v√† `world`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umXnbIqZ_xaj"
   },
   "outputs": [],
   "source": [
    "top_positive_bigrams = get_top_n_grams(df[df['label'] == 1]['clean_text'], ngram_range=(2, 2), n=20)\n",
    "top_negative_bigrams = get_top_n_grams(df[df['label'] == 0]['clean_text'], ngram_range=(2, 2), n=20)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in top_positive_bigrams], y=[word[0] for word in top_positive_bigrams])\n",
    "plt.title('Top 20 Bigrams in True News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Bigrams')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=[word[1] for word in top_negative_bigrams], y=[word[0] for word in top_negative_bigrams])\n",
    "plt.title('Top 20 Bigrams in Fake News')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Bigrams')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwTCEjKL_1ym"
   },
   "source": [
    "C·∫£ hai ƒë·ªÅu c√≥ s·ª± xu·∫•t hi·ªán m·∫°nh c·ªßa `trump`, `clinton`, v√† `united states`, nh∆∞ng True News t·∫≠p trung h∆°n v√†o c√°c thu·∫≠t ng·ªØ ch√≠nh th·ª©c (nh∆∞ `prime minister`, `supreme court`) v·ªõi t·∫ßn su·∫•t gi·∫£m ƒë·ªÅu, trong khi Fake News c√≥ th√™m c√°c t·ª´ li√™n quan ƒë·∫øn truy·ªÅn th√¥ng (nh∆∞ `twitter com`, `pic twitter`) v√† h√¨nh ·∫£nh (nh∆∞ `featured image`, `getty images`), cho th·∫•y s·ª± kh√°c bi·ªát v·ªÅ phong c√°ch v√† ngu·ªìn th√¥ng tin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M7q-XXi_4Ka"
   },
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vRxlXoUx_6OH"
   },
   "outputs": [],
   "source": [
    "true_reviews = df[df['label'] == 1]['clean_text']\n",
    "tfidf_vectorizer_true = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_true = tfidf_vectorizer_true.fit_transform(true_reviews)\n",
    "true_top_words = pd.DataFrame(tfidf_true.toarray(), columns=tfidf_vectorizer_true.get_feature_names_out()).mean().sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=true_top_words.values, y=true_top_words.index)\n",
    "plt.title('Top 20 TF-IDF Words in True News')\n",
    "plt.xlabel('TF-IDF Score')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJPaMzj5_7vh"
   },
   "outputs": [],
   "source": [
    "fake_reviews = df[df['label'] == 0]['clean_text']\n",
    "tfidf_vectorizer_fake = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_fake = tfidf_vectorizer_fake.fit_transform(fake_reviews)\n",
    "fake_top_words = pd.DataFrame(tfidf_fake.toarray(), columns=tfidf_vectorizer_fake.get_feature_names_out()).mean().sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=fake_top_words.values, y=fake_top_words.index)\n",
    "plt.title('Top 20 TF-IDF Words in Fake News')\n",
    "plt.xlabel('TF-IDF Score')\n",
    "plt.ylabel('Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiCvHjEM_-Vy"
   },
   "source": [
    "C·∫£ hai ƒë·ªÅu c√≥ s·ª± xu·∫•t hi·ªán m·∫°nh c·ªßa `trump`, `clinton`, `president`, v√† `said`, nh∆∞ng True News nh·∫•n m·∫°nh c√°c thu·∫≠t ng·ªØ h√†nh ch√≠nh (nh∆∞ `government`, `states`) v·ªõi ƒëi·ªÉm TF-IDF gi·∫£m ƒë·ªÅu, trong khi Fake News n·ªïi b·∫≠t v·ªõi c√°c t·ª´ nh∆∞ `hillary`, `obama`, v√† `russia`, g·ª£i √Ω s·ª± t·∫≠p trung v√†o c√°c c√° nh√¢n v√† s·ª± ki·ªán c·ª• th·ªÉ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGIhetpC_-3O"
   },
   "source": [
    "## Textual Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYT4cNW0_9l7"
   },
   "outputs": [],
   "source": [
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "df['polarity'] = df['clean_text'].apply(get_polarity)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='polarity', hue='label', multiple='stack', bins=50, kde=True)\n",
    "plt.title('Polarity Distribution by Label')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EE9sDhFnAB4H"
   },
   "source": [
    "Ph√¢n ph·ªëi c·ªßa c·∫£ hai nh√£n (0 v√† 1) t·∫≠p trung ch·ªß y·∫øu quanh gi√° tr·ªã Polarity g·∫ßn 0, r·∫•t √≠t b√†i vi·∫øt v·ªõi Polarity c·ª±c ƒëoan (d∆∞·ªõi -0.75 ho·∫∑c tr√™n 0.75) cho th·∫•y ph·∫ßn l·ªõn c√°c b√†i vi·∫øt c√≥ ƒë·ªô t√≠ch c·ª±c ho·∫∑c ti√™u c·ª±c trung b√¨nh. Nh√£n 0 (Fake News) c√≥ s·ªë l∆∞·ª£ng b√†i vi·∫øt cao h∆°n ƒë√°ng k·ªÉ so v·ªõi nh√£n 1 (True News)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9u6x4BsAEGc"
   },
   "outputs": [],
   "source": [
    "def get_subjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "df['subjectivity'] = df['clean_text'].apply(get_subjectivity)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='subjectivity', hue='label', multiple='stack', bins=50, kde=True)\n",
    "plt.title('Subjectivity Distribution by Label')\n",
    "plt.xlabel('Subjectivity')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1jZ5dMoAG47"
   },
   "source": [
    "Ph√¢n ph·ªëi c·ªßa c·∫£ hai nh√£n (0 v√† 1) t·∫≠p trung ch·ªß y·∫øu ·ªü gi√° tr·ªã Subjectivity t·ª´ 0 ƒë·∫øn 0.6, r·∫•t √≠t b√†i vi·∫øt l·ªõn h∆°n 0.6. Nh√£n 0 (Fake News) chi·∫øm ∆∞u th·∫ø t·ªïng th·ªÉ v√† c√≥ s·ªë l∆∞·ª£ng b√†i vi·∫øt cao v∆∞·ª£t tr·ªôi h∆°n nh√£n 1 ·ªü gi√° tr·ªã 0.1 v·ªõi kho·∫£ng 3.000 b√†i vi·∫øt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzshoVNnAIbU"
   },
   "outputs": [],
   "source": [
    "def get_flesch_kincaid(text):\n",
    "    return textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "df['readability_score'] = df['clean_text'].apply(get_flesch_kincaid)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='readability_score', hue='label', multiple='stack', bins=50, kde=True)\n",
    "plt.title('Readability Score Distribution for True vs Fake News')\n",
    "plt.xlabel('Flesch-Kincaid Grade Level')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.legend(labels=['Fake', 'True'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RAvmdnLAJ4i"
   },
   "source": [
    "Ph√¢n ph·ªëi c·ªßa c·∫£ tin th·∫≠t (nh√£n 1) v√† tin gi·∫£ (nh√£n 0) t·∫≠p trung ch·ªß y·∫øu ·ªü m·ª©c Flesch-Kincaid Grade Level t·ª´ 0 ƒë·∫øn 40, c·∫£ hai lo·∫°i tin ƒë·ªÅu c√≥ s·ªë l∆∞·ª£ng gi·∫£m m·∫°nh khi Flesch-Kincaid Grade Level tƒÉng tr√™n 40, v·ªõi r·∫•t √≠t b√†i vi·∫øt ·ªü m·ª©c tr√™n 80, cho th·∫•y c·∫£ hai lo·∫°i ƒë·ªÅu c√≥ m·ª©c ƒë·ªô d·ªÖ ƒë·ªçc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbhelCr9ANNs"
   },
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yq5lUCYvAOTx"
   },
   "outputs": [],
   "source": [
    "def count_punctuation(text, punct):\n",
    "    return text.count(punct)\n",
    "\n",
    "df['exclamation_count'] = df['clean_text'].apply(lambda x: count_punctuation(x, '!'))\n",
    "df['question_count'] = df['clean_text'].apply(lambda x: count_punctuation(x, '?'))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='exclamation_count', hue='label', multiple='stack', bins=30)\n",
    "plt.title('Exclamation Mark (!) Distribution by Label')\n",
    "plt.xlabel('Number of Exclamation Marks')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='question_count', hue='label', multiple='stack', bins=30)\n",
    "plt.title('Question Mark (?) Distribution by Label')\n",
    "plt.xlabel('Number of Question Marks')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ri4YRqhkARYd"
   },
   "outputs": [],
   "source": [
    "df = df[['text', 'label', 'clean_text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5FEHQyZ6BbZ"
   },
   "source": [
    "# Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWZ2HCyWR6zd"
   },
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.055661Z",
     "iopub.status.busy": "2025-06-17T12:59:54.055482Z",
     "iopub.status.idle": "2025-06-17T12:59:54.059413Z",
     "shell.execute_reply": "2025-06-17T12:59:54.058712Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.055648Z"
    },
    "id": "1dMwo7LNR-jI",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmUqkSlh6DBz"
   },
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.061793Z",
     "iopub.status.busy": "2025-06-17T12:59:54.061602Z",
     "iopub.status.idle": "2025-06-17T12:59:54.081312Z",
     "shell.execute_reply": "2025-06-17T12:59:54.080782Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.061779Z"
    },
    "id": "qaMNYg9OSBex",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = df['clean_text'].tolist()\n",
    "y = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.082186Z",
     "iopub.status.busy": "2025-06-17T12:59:54.081958Z",
     "iopub.status.idle": "2025-06-17T12:59:54.159538Z",
     "shell.execute_reply": "2025-06-17T12:59:54.158834Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.082171Z"
    },
    "id": "utsC--UE6iWA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=RANDOM_STATE, stratify=df['label'])\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cg7wwEteKlXO"
   },
   "source": [
    "* ƒê·∫øm s·ªë l∆∞·ª£ng nh√£n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.160556Z",
     "iopub.status.busy": "2025-06-17T12:59:54.160322Z",
     "iopub.status.idle": "2025-06-17T12:59:54.168358Z",
     "shell.execute_reply": "2025-06-17T12:59:54.167770Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.160536Z"
    },
    "id": "YZbh0IWl6v7y",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train label distribution: Counter({1: 24161, 0: 23742})\n",
      "Validation label distribution: Counter({1: 5178, 0: 5087})\n",
      "Test label distribution: Counter({1: 5178, 0: 5088})\n"
     ]
    }
   ],
   "source": [
    "print(\"Train label distribution:\", Counter(y_train))\n",
    "print(\"Validation label distribution:\", Counter(y_val))\n",
    "print(\"Test label distribution:\", Counter(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZMw5cAh7AJ9"
   },
   "source": [
    "## The necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.169117Z",
     "iopub.status.busy": "2025-06-17T12:59:54.168915Z",
     "iopub.status.idle": "2025-06-17T12:59:54.181891Z",
     "shell.execute_reply": "2025-06-17T12:59:54.181307Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.169104Z"
    },
    "id": "bgdhow5h7q4U",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "  label_description = {\"0\": \"Fake News\", \"1\": \"True News\"}\n",
    "  print(\"Classification report: \\n\", classification_report(y_true , y_pred))\n",
    "\n",
    "  print(\"Confusion matrix: \\n\")\n",
    "  conf_matrix = confusion_matrix(y_true , y_pred)\n",
    "  plt.figure(figsize=(10, 7))\n",
    "  sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=list(label_description.values()), yticklabels=list(label_description.values()))\n",
    "  plt.xlabel('Predicted Class')\n",
    "  plt.ylabel('True Class')\n",
    "  plt.title('Confusion Matrix')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.182722Z",
     "iopub.status.busy": "2025-06-17T12:59:54.182509Z",
     "iopub.status.idle": "2025-06-17T12:59:54.196106Z",
     "shell.execute_reply": "2025-06-17T12:59:54.195457Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.182705Z"
    },
    "id": "8aXKjXOxT37m",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_curves(train_loss, val_loss, val_acc, title=\"Learning Curve\"):\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # Loss\n",
    "    axes[0].plot(epochs, train_loss, label=\"Train loss\")\n",
    "    axes[0].plot(epochs, val_loss,   label=\"Val loss\")\n",
    "    axes[0].set_xlabel(\"Epoch\"); axes[0].set_ylabel(\"Loss\")\n",
    "    axes[0].set_title(\"Loss\"); axes[0].legend(); axes[0].grid(ls=\"--\", alpha=.4)\n",
    "\n",
    "    # Val accuracy\n",
    "    axes[1].plot(epochs, val_acc, label=\"Val acc\", color=\"tab:orange\")\n",
    "    axes[1].set_xlabel(\"Epoch\"); axes[1].set_ylabel(\"Accuracy\")\n",
    "    axes[1].set_title(\"Validation accuracy\")\n",
    "    axes[1].legend(); axes[1].grid(ls=\"--\", alpha=.4)\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.196872Z",
     "iopub.status.busy": "2025-06-17T12:59:54.196713Z",
     "iopub.status.idle": "2025-06-17T12:59:54.210411Z",
     "shell.execute_reply": "2025-06-17T12:59:54.209833Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.196860Z"
    },
    "id": "DL_sea3IK2Lp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_score, pos_label=1, title=\"ROC Curve\"):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score, pos_label=pos_label)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    ax.plot(fpr, tpr, color=\"tab:red\",\n",
    "            label=f\"User model (AUC = {roc_auc:.2f})\", lw=2)\n",
    "\n",
    "    # random\n",
    "    ax.plot([0, 1], [0, 1], \"k--\", lw=2)  # ƒê∆∞·ªùng ch√©o\n",
    "\n",
    "    # perfect\n",
    "    ax.plot([0, 0, 1], [0, 1, 1], color=\"green\",\n",
    "            label=\"Perfect model\", lw=1)\n",
    "\n",
    "    ax.set_xlim([0.0, 1.0]); ax.set_ylim([0.0, 1.02])\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(title)\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.grid(alpha=0.3, ls=\"--\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqMoPAZm6zHn"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZW9yBpQ629U"
   },
   "source": [
    "### Model ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bji6tbYjRwH7"
   },
   "outputs": [],
   "source": [
    "stemmer    = PorterStemmer()\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80nLEAP-R2Tl"
   },
   "outputs": [],
   "source": [
    "def tokenize_and_filter(text):\n",
    "    text = contractions.fix(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return [stemmer.stem(w)\n",
    "            for w in tokens\n",
    "            if w.lower() not in stop_words\n",
    "            and w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWun664OC7Rz"
   },
   "outputs": [],
   "source": [
    "def build_ml_model(X, y):\n",
    "    model = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            tokenizer=tokenize_and_filter,\n",
    "            lowercase=False,\n",
    "            preprocessor=None,\n",
    "            token_pattern=None,\n",
    "            ngram_range=(1, 2)\n",
    "        )),\n",
    "        (\"svc\", SVC(kernel='linear'))\n",
    "    ])\n",
    "\n",
    "    model.fit(X, y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x53pgZBdwTlm"
   },
   "outputs": [],
   "source": [
    "model = build_ml_model(X_train, y_train)\n",
    "\n",
    "pred = model.predict(X_val)\n",
    "evaluate(y_val, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKdI9Y_YREpb"
   },
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"tfidf__min_df\":      [2, 5, 10],\n",
    "    \"tfidf__max_df\":      [0.85, 0.9, 0.95],\n",
    "    \"tfidf__max_features\": [None, 50_000, 100_000],\n",
    "\n",
    "    \"svc__C\":            [0.1, 1, 2, 5],\n",
    "    \"svc__class_weight\": [None, \"balanced\"],\n",
    "}\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        tokenizer      = tokenize_and_filter,\n",
    "        lowercase      = False,   # ta ƒë√£ x·ª≠ l√Ω trong tokenizer\n",
    "        preprocessor   = None,\n",
    "        token_pattern  = None     # t·∫Øt regex m·∫∑c ƒë·ªãnh\n",
    "    )),\n",
    "    (\"svc\",  SVC(kernel=\"linear\", random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Set up GridSearchCV\n",
    "gridsearch = GridSearchCV(pipeline, param_grid, cv=5, scoring=\"f1\", verbose=1)\n",
    "\n",
    "# Find the best hyperparameters\n",
    "gridsearch.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters found and the best cross-validation score\n",
    "print(\"Best Parameters:\", gridsearch.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", gridsearch.best_score_)\n",
    "\n",
    "# Save the fitted GridSearchCV object to a pkl file\n",
    "with open('best_svm.pkl', 'wb') as file:\n",
    "    pickle.dump(gridsearch, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1wsUy2RHU7va"
   },
   "outputs": [],
   "source": [
    "# Load the GridSearchCV object from the pickle file\n",
    "with open('best_svm.pkl', 'rb') as file:\n",
    "    loaded_gridsearch = pickle.load(file)\n",
    "\n",
    "print(\"Best Parameters:\", loaded_gridsearch.best_params_)\n",
    "\n",
    "best_model = loaded_gridsearch.best_estimator_\n",
    "\n",
    "y_score = best_model.decision_function(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uWnMVxSh74a5"
   },
   "source": [
    "### Model DL c∆° b·∫£n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtwE6NRH8mJT"
   },
   "source": [
    "#### 1. Multilevel-CNN\n",
    "- B·∫Øt ƒë∆∞·ª£c ƒë·∫∑c tr∆∞ng t·ª´ t·ª´, c·ª•m t·ª´, c√¢u b·∫±ng c√°c kernel k√≠ch th∆∞·ªõc kh√°c nhau (3, 4, 5,...).\n",
    "- Ph√π h·ª£p v·ªõi d·ªØ li·ªáu d√†i + ƒëa d·∫°ng, kh√¥ng ph·ª• thu·ªôc v√†o th·ª© t·ª± qu√° d√†i nh∆∞ RNN.\n",
    "- Hu·∫•n luy·ªán nhanh h∆°n LSTM, ƒë·ªô ch√≠nh x√°c cao h∆°n CNN ƒë∆°n thu·∫ßn.\n",
    "\n",
    "#### 2. CNN + BiLSTM\n",
    "- CNN tr√≠ch ƒë·∫∑c tr∆∞ng c·ª•c b·ªô, sau ƒë√≥ BiLSTM hi·ªÉu ng·ªØ c·∫£nh hai chi·ªÅu (tr∆∞·ªõc v√† sau).\n",
    "- Ph√π h·ª£p cho d·ªØ li·ªáu c√≥ logic tuy·∫øn t√≠nh (nh∆∞ tin t·ª©c).\n",
    "- ƒê·ªô ch√≠nh x√°c cao, tuy ch·∫≠m h∆°n Multilevel-CNN ch√∫t nh∆∞ng v·∫´n t·ªët n·∫øu t·ªëi ∆∞u ƒë√∫ng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWDeUpSR8oJu"
   },
   "source": [
    "#### Multilevel-CNN\n",
    "\n",
    "**Ki·∫øn tr√∫c g·ª£i √Ω:**\n",
    "\n",
    "Input (chu·ªói vƒÉn b·∫£n)\n",
    "‚Üí Embedding Layer\n",
    "‚Üí Conv1D (kernel size 3) ‚Üí GlobalMaxPool\n",
    "‚Üí Conv1D (kernel size 4) ‚Üí GlobalMaxPool\n",
    "‚Üí Conv1D (kernel size 5) ‚Üí GlobalMaxPool\n",
    "‚Üí Concatenate\n",
    "‚Üí Dense layers ‚Üí Dropout (ch∆∞a c√≥ trong code)\n",
    "‚Üí Output (Sigmoid / Softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjg0PYCK8lid"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "# Fit tokenizer tr√™n d·ªØ li·ªáu train\n",
    "tokenizer = Tokenizer(num_words=5000) # gi·ªØ l·∫°i 5000 t·ª´ ph·ªï bi·∫øn nh·∫•t\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# Convert vƒÉn b·∫£n th√†nh c√¢u v√† padding\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=512, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=512, padding='post', truncating='post')\n",
    "\n",
    "# Convert to tensor\n",
    "X_train_tensor = torch.tensor(X_train_pad, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_pad, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Dataset & DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    '''\n",
    "    T·∫°o custom Dataset t·ª´ d·ªØ li·ªáu ƒë√£ padding v√† label.\n",
    "    '''\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TextDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataset = TextDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a model\n",
    "class MultilevelCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes=1):\n",
    "        super(MultilevelCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim) # Embedding layer\n",
    "        self.conv3 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=3)\n",
    "        self.conv4 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=4)\n",
    "        self.conv5 = nn.Conv1d(in_channels=embed_dim, out_channels=32, kernel_size=5)\n",
    "        self.fc = nn.Linear(32*3, 10)\n",
    "        self.out = nn.Linear(10, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch_size, seq_len, embed_dim)\n",
    "        x = x.permute(0, 2, 1) # (batch_size, embed_dim, seq_len)\n",
    "\n",
    "        x1 = F.relu(self.conv3(x)) # Conv1d v·ªõi kernel_size = 3\n",
    "        x2 = F.relu(self.conv4(x)) # Conv1d v·ªõi kernel_size = 4\n",
    "        x3 = F.relu(self.conv5(x)) # Conv1d v·ªõi kernel_size = 5\n",
    "\n",
    "        x1 = F.max_pool1d(x1, x1.size(2)).squeeze(2)\n",
    "        x2 = F.max_pool1d(x2, x2.size(2)).squeeze(2)\n",
    "        x3 = F.max_pool1d(x3, x3.size(2)).squeeze(2)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), 1) # N·ªëi l·∫°i c√°c features\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = torch.sigmoid(self.out(x)) # Binary classification\n",
    "\n",
    "        return x\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = MultilevelCNN(vocab_size=5000, embed_dim=16).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 2\n",
    "wait = 0\n",
    "num_epochs = 20\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=True)\n",
    "    for batch_X, batch_y in loop:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X).squeeze(1)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    # === Validation sau m·ªói epoch ===\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    val_loss_total = 0\n",
    "    total = 0\n",
    "    val_loop = tqdm(test_loader, desc=f\"Epoch {epoch+1} [Val]\", leave=True)\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loop:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X).squeeze(1)\n",
    "            val_loss = criterion(outputs, batch_y)\n",
    "            val_loss_total += val_loss.item()\n",
    "\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "            val_loop.set_postfix(val_loss=val_loss.item())\n",
    "\n",
    "    val_acc = correct / total\n",
    "    val_loss_avg = val_loss_total / len(test_loader)\n",
    "    total_loss_avg = total_loss / len(test_loader)\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {total_loss_avg:.4f} | Val Loss: {val_loss_avg:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "    # === Early stopping check ===\n",
    "    if val_loss_avg < best_val_loss:\n",
    "        best_val_loss = val_loss_avg\n",
    "        wait = 0\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJ86o9aT82Cz"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "def predict_text(texts):\n",
    "    model.eval()\n",
    "    seq = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(seq, maxlen=512, padding='post', truncating='post')\n",
    "    tensor = torch.tensor(padded, dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tensor)\n",
    "        probs = outputs.cpu().numpy()\n",
    "        return probs, (probs > 0.5).astype(int)\n",
    "\n",
    "probs, preds = predict_text(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUnO7diO76eU"
   },
   "source": [
    "### Model DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.211366Z",
     "iopub.status.busy": "2025-06-17T12:59:54.211146Z",
     "iopub.status.idle": "2025-06-17T12:59:54.228355Z",
     "shell.execute_reply": "2025-06-17T12:59:54.227704Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.211352Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "epochs = 5\n",
    "patience = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.229155Z",
     "iopub.status.busy": "2025-06-17T12:59:54.228941Z",
     "iopub.status.idle": "2025-06-17T12:59:54.242534Z",
     "shell.execute_reply": "2025-06-17T12:59:54.242021Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.229141Z"
    },
    "id": "vnbDwTZw62TJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_tokenizer_and_model(model_name: str, num_labels: int):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.243525Z",
     "iopub.status.busy": "2025-06-17T12:59:54.243218Z",
     "iopub.status.idle": "2025-06-17T12:59:54.258012Z",
     "shell.execute_reply": "2025-06-17T12:59:54.257433Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.243505Z"
    },
    "id": "Q-IbcO-P6B_e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.258887Z",
     "iopub.status.busy": "2025-06-17T12:59:54.258706Z",
     "iopub.status.idle": "2025-06-17T12:59:54.272142Z",
     "shell.execute_reply": "2025-06-17T12:59:54.271591Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.258873Z"
    },
    "id": "KFc31g3v8DZn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DataLoaderBuilder:\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=True, num_workers=2):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def get_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T12:59:54.273072Z",
     "iopub.status.busy": "2025-06-17T12:59:54.272846Z",
     "iopub.status.idle": "2025-06-17T12:59:54.294798Z",
     "shell.execute_reply": "2025-06-17T12:59:54.294184Z",
     "shell.execute_reply.started": "2025-06-17T12:59:54.273050Z"
    },
    "id": "keYc2bbk8CfC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, model_name, lr=2e-5, epochs=5, patience=2, device='cuda'):\n",
    "        self.model = model.to(device)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.model_name = model_name\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.patience = patience\n",
    "        self.device = device\n",
    "        self.optimizer = AdamW(self.model.parameters(), lr=lr)\n",
    "        self.scheduler = get_scheduler(\"linear\", self.optimizer, num_warmup_steps=0,\n",
    "                                       num_training_steps=len(train_loader) * epochs)\n",
    "        \n",
    "        # Track history\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "            outputs = self.model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_loader, desc=\"Validating\"):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                total_loss += loss.item()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        avg_loss = total_loss / len(self.val_loader)\n",
    "        return acc, avg_loss, all_preds, all_labels\n",
    "\n",
    "    def train(self):\n",
    "        best_loss = float('inf')\n",
    "        stop_count = 0\n",
    "        save_path = os.path.join(\"/kaggle/working\", f\"{self.model_name}_best.pt\")\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            print(f\"\\nEpoch {epoch}/{self.epochs}\")\n",
    "            train_loss = self.train_one_epoch()\n",
    "            val_acc, val_loss, _, _ = self.evaluate()\n",
    "\n",
    "            self.train_losses.append(train_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.val_accuracies.append(val_acc)\n",
    "\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss:   {val_loss:.4f} | Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                stop_count = 0\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "            else:\n",
    "                stop_count += 1\n",
    "                if stop_count >= self.patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "\n",
    "        # Reload best model\n",
    "        self.model.load_state_dict(torch.load(save_path))\n",
    "        \n",
    "        # L∆∞u l·∫°i l·ªãch s·ª≠ train/val\n",
    "        log_df = pd.DataFrame({\n",
    "            \"epoch\": list(range(1, len(self.train_losses) + 1)),\n",
    "            \"train_loss\": self.train_losses,\n",
    "            \"val_loss\": self.val_losses,\n",
    "            \"val_acc\": self.val_accuracies\n",
    "        })\n",
    "        log_df.to_csv(f\"/kaggle/working/{self.model_name}_training_log.csv\", index=False)\n",
    "        print(\"Training log saved.\")\n",
    "\n",
    "        return self.train_losses, self.val_losses, self.val_accuracies\n",
    "\n",
    "    def test_model(self, test_loader):\n",
    "        \"\"\"\n",
    "        ƒê√°nh gi√° m√¥ h√¨nh ƒë√£ l∆∞u tr√™n t·∫≠p test.\n",
    "        Tr·∫£ v·ªÅ: acc, loss, preds, true_labels, probs\n",
    "        \"\"\"\n",
    "        save_path = os.path.join(\"/kaggle/working\", f\"{self.model_name}_best.pt\")\n",
    "        self.model.load_state_dict(torch.load(save_path))\n",
    "        self.model.eval()\n",
    "\n",
    "        all_preds, all_labels, all_probs = [], [], []\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                preds = torch.argmax(probs, dim=-1)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "                all_probs.extend(probs[:, 1].cpu().numpy() if probs.shape[1] > 1 else probs[:, 0].cpu().numpy())\n",
    "\n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        # Save for later analysis\n",
    "        results_df = pd.DataFrame({\n",
    "            \"true_label\": all_labels,\n",
    "            \"pred_label\": all_preds,\n",
    "            \"prob_class1\": all_probs\n",
    "        })\n",
    "        results_path = f\"/kaggle/working/{self.model_name}_test_results.csv\"\n",
    "        results_df.to_csv(results_path, index=False)\n",
    "        print(f\"Test results saved to {results_path}\")\n",
    "\n",
    "        return acc, avg_loss, all_preds, all_labels, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:14:14.872056Z",
     "iopub.status.busy": "2025-06-17T13:14:14.871727Z",
     "iopub.status.idle": "2025-06-17T13:14:14.881556Z",
     "shell.execute_reply": "2025-06-17T13:14:14.880817Z",
     "shell.execute_reply.started": "2025-06-17T13:14:14.872035Z"
    },
    "id": "csCM3MzT8HMD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_model(model_name, batch_size):\n",
    "    global df, learning_rate, epochs, patience, X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "    print(\"========== LOAD TOKENIZER & MODEL ==========\")\n",
    "    tokenizer, model = get_tokenizer_and_model(model_name, num_labels=len(set(df[\"label\"])))\n",
    "\n",
    "    print(\"========== CREATE DATASETS ==========\")\n",
    "    train_dataset = TextClassificationDataset(X_train, y_train, tokenizer)\n",
    "    val_dataset = TextClassificationDataset(X_val, y_val, tokenizer)\n",
    "    test_dataset = TextClassificationDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "    print(\"========== CREATE DATALOADERS ==========\")\n",
    "    train_loader = DataLoaderBuilder(train_dataset, batch_size=batch_size, shuffle=True).get_dataloader()\n",
    "    val_loader = DataLoaderBuilder(val_dataset, batch_size=batch_size, shuffle=False).get_dataloader()\n",
    "    test_loader = DataLoaderBuilder(test_dataset, batch_size=batch_size, shuffle=False).get_dataloader()\n",
    "\n",
    "    print(\"========== INITIALIZE TRAINER ==========\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        model_name=model_name,\n",
    "        lr=learning_rate,\n",
    "        epochs=epochs,\n",
    "        patience=patience,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"\\tTRAINING MODEL: {model_name}\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    train_losses, val_losses, val_accuracies = trainer.train()\n",
    "\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"\\tEVALUATION ON TEST SET\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    test_acc, test_loss, test_preds, test_true_labels, test_probs = trainer.test_model(test_loader)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_true_labels, test_preds, target_names=[str(i) for i in sorted(df['label'].unique())]))\n",
    "\n",
    "    cm = confusion_matrix(test_true_labels, test_preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=sorted(df['label'].unique()),\n",
    "                yticklabels=sorted(df['label'].unique()))\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Test Accuracy: {test_acc:.4f} | Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "    # Tr·∫£ v·ªÅ d·ªØ li·ªáu cho vi·ªác v·∫Ω v·ªÅ sau ho·∫∑c l∆∞u log th√™m\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accuracies\": val_accuracies,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_preds\": test_preds,\n",
    "        \"test_labels\": test_true_labels,\n",
    "        \"test_probs\": test_probs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T13:14:17.464179Z",
     "iopub.status.busy": "2025-06-17T13:14:17.463569Z"
    },
    "id": "bzQXPI5e8Kob",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LOAD TOKENIZER & MODEL ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== CREATE DATASETS ==========\n",
      "========== CREATE DATALOADERS ==========\n",
      "========== INITIALIZE TRAINER ==========\n",
      "\n",
      "--------------------------------------------------\n",
      "\tTRAINING MODEL: bert-base-uncased\n",
      "--------------------------------------------------\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|‚ñè         | 11/749 [00:18<19:52,  1.62s/it]"
     ]
    }
   ],
   "source": [
    "results = run_model(\"bert-base-uncased\", batch_size=64)\n",
    "\n",
    "plot_curves(results[\"train_losses\"], results[\"val_losses\"], results[\"val_accuracies\"])\n",
    "plot_roc_curve(results[\"test_labels\"], results[\"test_probs\"])\n",
    "evaluate(results[\"test_labels\"], results[\"test_preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xPFt6Fuj8MEq"
   },
   "outputs": [],
   "source": [
    "results_2 = run_model(\"roberta-base\", batch_size=64)\n",
    "\n",
    "plot_curves(results_2[\"train_losses\"], results_2[\"val_losses\"], results_2[\"val_accuracies\"])\n",
    "plot_roc_curve(results_2[\"test_labels\"], results_2[\"test_probs\"])\n",
    "evaluate(results_2[\"test_labels\"], results_2[\"test_preds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VelNmI928Opg"
   },
   "outputs": [],
   "source": [
    "results_3 = run_model(model_name=\"xlnet-base-cased\", batch_size=32)\n",
    "\n",
    "plot_curves(results_3[\"train_losses\"], results_3[\"val_losses\"], results_3[\"val_accuracies\"])\n",
    "plot_roc_curve(results_3[\"test_labels\"], results_3[\"test_probs\"])\n",
    "evaluate(results_3[\"test_labels\"], results_3[\"test_preds\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "nW7Xan1n6VtR"
   ],
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7615110,
     "sourceId": 12096450,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
